Markdown

# ğŸ“Š Visual Diversity Evaluation for Image Datasets

> **Implementation of FineVision's Visual Diversity Metric**  
> Quantifying dataset quality using SSCD embeddings, Effective Rank & Participation Ratio

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ Motivation

This repository implements the **visual diversity measurement algorithm** used in HuggingFace's [FineVision](https://huggingface.co/spaces/HuggingFaceM4/FineVision) project for evaluating MLLM (Multimodal Large Language Model) SFT datasets.

### Why This Matters
- âœ… **Quantify Dataset Quality**: Transform subjective assessment into objective metrics
- âœ… **Detect Bias**: Automatically identify repetitive patterns (e.g., tracking datasets with repeated backgrounds)
- âœ… **Optimize Data Augmentation**: Understand which directions need more diversity
- âœ… **Active Learning**: Use diversity as a criterion for core-set selection

---

## ğŸ”¬ Algorithm Overview

### Step 1: SSCD Embedding Extraction
- **Model**: [SSCD (Self-Supervised Copy Detection)](https://github.com/facebookresearch/sscd-copy-detection) by Meta AI
- **Output**: 512-dimensional embedding vectors per image

### Step 2: Diversity Calculation Pipeline
Compute Covariance Matrix
â†’ Analyze the directional spread of data

Eigenvalue Decomposition
â†’ Extract principal components and their magnitudes

Calculate Effective Rank
â†’ Entropy-based measure of directional diversity
Effective Rank = exp(Entropy)

Calculate Participation Ratio
â†’ Measure how evenly variance is distributed
PR = (Î£Î»áµ¢)Â² / Î£(Î»áµ¢Â²)

Final Diversity Score
â†’ Geometric Mean(Effective Rank_normalized, Participation Ratio_normalized)

text


### Reference Papers
- [The Effective Rank: A Measure of Effective Dimensionality](https://www.eurasip.org/Proceedings/Eusipco/Eusipco2007/Papers/a5p-h05.pdf) (EUSIPCO 2007)
- [On the Importance of Single Directions for Generalization](https://arxiv.org/abs/1803.06959) (ICLR 2018)

---

## ğŸš€ Key Features

- âœ… **Multi-GPU Support**: Powered by `torch.nn.DataParallel`
- âœ… **Memory Efficient**: Local cache system (`.npy`) handles large-scale datasets (2.4M+ images)
- âœ… **Flexible Deployment**: CPU, Single GPU, Multi-GPU compatibility
- âœ… **Fast Processing**: Configurable batch processing with optimized throughput

---

## ğŸ“¦ Installation

### Requirements
```bash
pip install -r requirements.txt
Core Dependencies
Python >= 3.8
PyTorch >= 2.0.0
torchvision >= 0.15.0
numpy >= 1.24.0
scipy >= 1.10.0
Pillow >= 9.5.0
tqdm >= 4.65.0
pyyaml >= 6.0
ğŸ® Quick Start
Basic Usage
Python

from embedders.sscd_embedding import SSCDEmbedder
from diversity.diversity_calculation import DiversityCalculator

# Step 1: Extract SSCD embeddings
embedder = SSCDEmbedder(device='cuda', batch_size=32)
embeddings = embedder.extract('/path/to/dataset/')

# Step 2: Calculate diversity score
calculator = DiversityCalculator()
score = calculator.calculate(embeddings)

print(f"Diversity Score: {score:.6f}")
Using Configuration Files
Bash

# CPU mode
python test.py --config configuration/config_cpu.yaml

# Single GPU
python test.py --config configuration/config_specific_gpu.yaml

# Multi-GPU
python test.py --config configuration/config_specific_multi_gpu.yaml

# Large dataset with local cache
python test.py --config configuration/config_specific_gpu_local_cache.yaml
ğŸ“Š Benchmark Results
Comparison with FineVision Baselines
Dataset	Images	Diversity Score	Rating
FineVision	17.3M	0.500	â­â­â­â­â­ Very Good
Cambrian-7M	5.4M	0.458	â­â­â­â­ Good
M4-Instruct	2.48M	0.413	â­â­â­â­ Good
Cauldron	2.0M	0.400	â­â­â­â­ Good
LLaVa-Vision	2.5M	0.298	â­â­â­ Normal
Evaluation on 9 Public Datasets
Dataset	Images	Classes	Task	Diversity Score	Rating
Pascal VOC	17,125	20	Classification	0.885	â­â­â­â­â­
V3Det	212,917	13,204	Detection	0.879	â­â­â­â­â­
WiderFace	16,106	1	Face Detection	0.813	â­â­â­â­â­
CrowdHuman	23,740	2	Human Detection	0.758	â­â­â­â­â­
M4-Instruct	2,481,646	Multi	MLLM Instruction	0.413	â­â­â­â­
RVSD	8,404	80 scenes	DeSnowing	0.293	â­â­
SeaDroneSee	14,227	6	Maritime Detection	0.183	â­
DanceTrack	38,551	1	Human Tracking	0.145	â­
R7_Tracking	6,000	1	Sports Tracking	0.071	â­
Score Interpretation Guide
text

Diversity Score >= 0.50        : â­â­â­â­â­ Very Good
                                 - FineVision benchmark level
                                 - Optimal for large-scale MLLM training

0.40 <= Score < 0.50           : â­â­â­â­ Good
                                 - Cambrian-7M level
                                 - Suitable for general MLLM training

0.30 <= Score < 0.40           : â­â­â­ Normal
                                 - LLaVa-Vision level
                                 - Consider filtering or augmentation

0.20 <= Score < 0.30           : â­â­ Low
                                 - Potential bias or duplication issues

Score < 0.20                   : â­ Very Low
                                 - Quality inspection required
ğŸ’¡ Key Insights
1. Object Detection Datasets
Most achieve Very Good diversity (score > 0.7)
V3Det: 13,204 classes â†’ Maintains 0.879 diversity despite 213K images
2. Tracking Datasets
Repetitive backgrounds lead to Very Low diversity (0.071 ~ 0.183)
R7_Tracking: 3 backgrounds Ã— 2000 frames â†’ Severe bias (0.071)
3. MLLM Datasets
M4-Instruct: Lower than FineVision (0.5) but still Good (0.413)
Category diversity significantly impacts the score
4. Interesting Cases
RVSD: 80 locations â†’ Higher diversity (0.293) despite being a tracking dataset
SeaDroneSee: Repetitive maritime background â†’ Lower diversity (0.183) despite detection task
ğŸ—‚ Project Structure
text

visual-diversity-evaluation/
â”œâ”€â”€ configuration/              # YAML configuration files
â”‚   â”œâ”€â”€ config_cpu.yaml
â”‚   â”œâ”€â”€ config_specific_gpu.yaml
â”‚   â”œâ”€â”€ config_specific_multi_gpu.yaml
â”‚   â””â”€â”€ config_specific_gpu_local_cache.yaml
â”‚
â”œâ”€â”€ data_loaders/
â”‚   â””â”€â”€ custom_dataset.py      # Custom image dataset loader
â”‚
â”œâ”€â”€ embedders/
â”‚   â””â”€â”€ sscd_embedding.py      # SSCD embedding extractor
â”‚
â”œâ”€â”€ diversity/
â”‚   â””â”€â”€ diversity_calculation.py  # Diversity metric implementation
â”‚
â”œâ”€â”€ utils.py                    # Utility functions
â”œâ”€â”€ test.py                     # Main evaluation script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ¯ Use Cases
1. Dataset Quality Assessment
Python

# Evaluate diversity of your custom dataset
score = evaluate_diversity('/path/to/my_dataset/')
print(f"Dataset Quality Score: {score:.3f}")
2. Data Augmentation Guidance
Python

# Analyze which directions need augmentation
effective_rank, participation_ratio = get_diversity_components(embeddings)
3. Active Learning Core-Set Selection
Python

# Select diverse samples for annotation
selected_indices = select_diverse_samples(embeddings, k=1000)
4. Multi-Dataset Comparison
Python

# Compare multiple datasets
scores = {
    'dataset_A': evaluate_diversity('/path/A/'),
    'dataset_B': evaluate_diversity('/path/B/'),
}
ğŸ“ˆ Performance Benchmarks
Environment	Batch Size	Throughput (imgs/sec)	Memory
CPU (16 cores)	4	~5	8GB RAM
RTX 2080 (Single)	32	~120	8GB VRAM
RTX 2080 (Ã—4)	128	~400	32GB VRAM
A100 (Single)	64	~300	40GB VRAM
Large-Scale Dataset Handling:

M4-Instruct (2.48M images) â†’ Uses local cache (.npy format)
Batch size 4, Single RTX 2080 â†’ ~8 hours processing time
ğŸ”§ Advanced Configuration
Multi-GPU Setup
YAML

# configuration/config_specific_multi_gpu.yaml
device: 'cuda'
gpu_ids: [0, 1, 2, 3]
batch_size: 128
use_data_parallel: true
num_workers: 8
Local Cache for Large Datasets
YAML

# configuration/config_specific_gpu_local_cache.yaml
use_local_cache: true
cache_dir: './results/embeddings/'
cache_format: 'npy'
overwrite_cache: false
ğŸ¤ Contributing
Contributions are welcome! Please feel free to:

Submit bug reports or feature requests via Issues
Create Pull Requests for improvements
Share your evaluation results on new datasets
ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™ Acknowledgements
This work builds upon:

FineVision - HuggingFace M4 Team
SSCD - Meta AI Research
Effective Rank - Roy & Vetterli (EUSIPCO 2007)
Participation Ratio - Morcos et al. (ICLR 2018)
